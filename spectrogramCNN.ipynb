{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:54.692740Z",
     "start_time": "2025-07-30T20:21:49.854743Z"
    }
   },
   "source": [
    "import os\n",
    "from doctest import debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sympy.codegen.cnodes import static\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:54.704843Z",
     "start_time": "2025-07-30T20:21:54.701552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data paths\n",
    "mixture_directory_test = \"data/mixture/test\"\n",
    "mixture_directory_train = \"data/mixture/train\"\n",
    "vocal_directory_test = \"data/vocal/test\"\n",
    "vocal_directory_train = \"data/vocal/train\""
   ],
   "id": "452a325d3e98dbb7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:54.719123Z",
     "start_time": "2025-07-30T20:21:54.715054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parameters to play around with to better optimize training\n",
    "patch_size = 128\n",
    "stride = 64\n",
    "batch_size = 1\n",
    "epochs = 20\n",
    "model_save_path = \"vocal_isolator.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # will utilize GPU if possible to train/test"
   ],
   "id": "4313de376c9ccb55",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:54.735328Z",
     "start_time": "2025-07-30T20:21:54.725317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, mixed_dir, vocal_dir, use_magnitude=True):\n",
    "        self.mixed_map = self._make_file_map(mixed_dir, prefix='mix_')\n",
    "        self.vocals_map = self._make_file_map(vocal_dir, prefix='vocal_')\n",
    "        \n",
    "        # error check with available song keys\n",
    "        self.keys = sorted(list(set(self.mixed_map.keys()) & set(self.vocals_map.keys())))\n",
    "        assert self.keys, \"No matching files found between mixed and vocal directories\"\n",
    "        \n",
    "        self.pairs = [(self.mixed_map[k], self.vocals_map[k]) for k in self.keys]\n",
    "        self.use_magnitude = use_magnitude\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_file_map(directory, prefix):\n",
    "        file_map = {}\n",
    "        for path in glob.glob(os.path.join(directory, \"*.npy\")):\n",
    "            filename = os.path.basename(path)\n",
    "            if filename.startswith(prefix):\n",
    "                key = filename[len(prefix):]  # strip the prefix\n",
    "                file_map[key] = path\n",
    "        return file_map\n",
    "    \n",
    "    # @staticmethod\n",
    "    # def _load_excel_sheet(filepath):\n",
    "    #     real = pd.read_excel(filepath, sheet_name='Re_X', header=None).values.astype(np.float32)\n",
    "    #     imaginary = pd.read_excel(filepath, sheet_name='Im_X', header=None).values.astype(np.float32)\n",
    "    #     fs = pd.read_excel(filepath, sheet_name='Sampling Rate', header=None).values[0][0]\n",
    "    #     \n",
    "    #     complex_stft = real + 1j*imaginary\n",
    "    #     return complex_stft, fs\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_npy(filepath):\n",
    "        return np.load(filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _pad_to_multiple_16(tensor, multiple=16):\n",
    "        _, h, w = tensor.shape\n",
    "        pad_h = (multiple - h % multiple) % multiple\n",
    "        pad_w = (multiple - w % multiple) % multiple\n",
    "        return F.pad(tensor, (0, pad_w, 0, pad_h))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs) #length of le dataset?BRO THIS IS WH Y IM NOT IN COMPUTER ENGINE\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        mixed_path, vocals_path = self.pairs[index]\n",
    "\n",
    "        mixed_stft = self._load_npy(mixed_path)\n",
    "        vocals_stft = self._load_npy(vocals_path)\n",
    "        \n",
    "        if self.use_magnitude:\n",
    "            mixed_mag = np.abs(mixed_stft)\n",
    "            vocals_mag = np.abs(vocals_stft)\n",
    "            \n",
    "            mixed_mag = (mixed_mag - mixed_mag.min()) / (mixed_mag.max() - mixed_mag.min() + 1e-8)\n",
    "            vocals_mag = (vocals_mag - vocals_mag.min()) / (vocals_mag.max() - vocals_mag.min() + 1e-8)\n",
    "            \n",
    "            mixed_tensor = torch.tensor(mixed_mag)\n",
    "            vocals_tensor = torch.tensor(vocals_mag)\n",
    "            \n",
    "            mixed_tensor = self._pad_to_multiple_16(mixed_tensor)\n",
    "            vocals_tensor = self._pad_to_multiple_16(vocals_tensor)\n",
    "            \n",
    "            return mixed_tensor, vocals_tensor\n",
    "\n",
    "        else:  # if training on complex values\n",
    "            return torch.tensor(mixed_stft), torch.tensor(vocals_stft)\n",
    "        \n",
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads all spectrograms in the batch to the maximum height and width in that batch.\n",
    "    \"\"\"\n",
    "    max_h = max(item[0].shape[1] for item in batch)\n",
    "    max_w = max(item[0].shape[2] for item in batch)\n",
    "    \n",
    "    mixed_padded = []\n",
    "    vocals_padded = []\n",
    "    for mixed, vocals in batch:\n",
    "        mixed_padded.append(F.pad(mixed, (0, max_w - mixed.shape[2], 0, max_h - mixed.shape[1])))\n",
    "        vocals_padded.append(F.pad(vocals, (0, max_w - vocals.shape[2], 0, max_h - vocals.shape[1])))\n",
    "\n",
    "    return torch.stack(mixed_padded), torch.stack(vocals_padded)\n"
   ],
   "id": "af2e915cab35f574",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:55.041678Z",
     "start_time": "2025-07-30T20:21:55.034671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UNet(nn.Module):  #TODO: Should we be using a CNN or a UNET?\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # 3x3 filter for feature detection\n",
    "                nn.BatchNorm2d(out_channels),  # stabilize and speeds up training\n",
    "                nn.ReLU(inplace=True),  # apply non-linearity for complex pattern learning\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        # Encoding - reduce spatial dimensions and abstract features so model can understand\n",
    "        self.encoder1 = conv_block(2, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # reduce resolution by 2 to allow for larger context\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.center = conv_block(512, 1024)  # decision hub - learns what high level features are\n",
    "        \n",
    "        #Decoding - up sample and reconstruct the isolated vocals\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "            \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 2, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )  # reduce back to 1 channel (vocal spectrogram)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool1(e1))\n",
    "        e3 = self.encoder3(self.pool2(e2))\n",
    "        e4 = self.encoder4(self.pool3(e3))\n",
    "        center = self.center(self.pool4(e4))\n",
    "        \n",
    "        # decoding\n",
    "        d4 = self.dec4(torch.cat([self.up4(center), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = self.final(d1)\n",
    "        \n",
    "        return out"
   ],
   "id": "c1aa36edb170aa76",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:55.082383Z",
     "start_time": "2025-07-30T20:21:55.078904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# helper functions for training function\n",
    "def compute_sdr(target, prediction):\n",
    "    noise = target - prediction\n",
    "    return 10 * torch.log10(torch.sum(target ** 2) / (torch.sum(noise ** 2) + 1e-8))\n",
    "\n",
    "def save_prediction_as_npy(prediction, filename):\n",
    "    prediction_np = prediction.squeeze().cpu().detach().numpy()\n",
    "    np.save(filename, prediction_np)\n",
    "    print(f\"Saved prediction to {filename}\")"
   ],
   "id": "1f601ca7cf94cf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:21:55.100154Z",
     "start_time": "2025-07-30T20:21:55.093606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # training function to be called\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, checkpoint_path=\"vocal_isolator.pth\"):\n",
    "    print(\"Starting training loop...\")\n",
    "    # os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    # model.train()\n",
    "    # checkpoint_interval = 5\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} starting...\")\n",
    "        # TRAINING\n",
    "        print(\"setting to training mode\")\n",
    "        model.train()  # set to training mode\n",
    "        running_loss = 0.0\n",
    "        # num_batches = 0\n",
    "        debug_num = epoch+1\n",
    "\n",
    "        for mixed, vocals in train_loader:\n",
    "            # print(\"moved to GPU\")\n",
    "            # Move to GPU\n",
    "            mixed = mixed.to(device)\n",
    "            vocals = vocals.to(device)\n",
    "            # print(\"forward pass starting\")\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"creating output from model\")\n",
    "            outputs = model(mixed)\n",
    "            loss = criterion(outputs, vocals)\n",
    "            \n",
    "            # Backward pass\n",
    "            # print(\"backward pass starting\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            debug_num += 0.01\n",
    "            print(debug_num)\n",
    "\n",
    "        # Calculate average loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # VALIDATION\n",
    "        print(\"validation beginning\")\n",
    "        model.eval()   # set to eval mode for validation\n",
    "        val_loss, mae, sdr = 0.0, 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for mixed, vocals in val_loader:\n",
    "                mixed, vocals = mixed.to(device), vocals.to(device)\n",
    "                # print(\"getting output from mixed for validation\")\n",
    "                outputs = model(mixed)\n",
    "                # print(\"calculating loss\")\n",
    "                val_loss += criterion(outputs, vocals).item()\n",
    "                # print(\"calculating MAE\")\n",
    "                mae += F.l1_loss(outputs, vocals).item()\n",
    "                # print(\"calculating SDR\")\n",
    "                sdr += compute_sdr(outputs, vocals)\n",
    "        val_loss /= len(val_loader)\n",
    "        mae /= len(val_loader)\n",
    "        sdr /= len(val_loader)\n",
    "        print(f\"Validation: Loss = {val_loss:.4f}, MAE = {mae:.4f}, SDR = {sdr:.4f} dB\")\n",
    "        \n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "        # save predicted xlsx\n",
    "        mixed_sample, _ = next(iter(val_loader))\n",
    "        mixed_sample = mixed_sample.to(device)\n",
    "        predicted = model(mixed_sample[:1])\n",
    "        save_prediction_as_npy(predicted, f\"predicted_epoch_{epoch}.npy\")\n",
    "    \n",
    "    print(f\"Training complete, model saved at {checkpoint_path}\")\n",
    "    return model"
   ],
   "id": "ef8199a5b692ad69",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T20:33:33.767048Z",
     "start_time": "2025-07-30T20:21:55.107748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training block\n",
    "train_dataset = SpectrogramDataset(mixed_dir=\"data/mixture/train\", vocal_dir=\"data/vocal/train\")\n",
    "val_dataset = SpectrogramDataset(mixed_dir=\"data/mixture/val\", vocal_dir=\"data/vocal/val\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn, num_workers=0)\n",
    "\n",
    "print(\"Testing dataset loading...\")\n",
    "for i in range(min(3, len(train_dataset))):\n",
    "    x,y = train_dataset[i]\n",
    "    print(f\"Sample {i} : x shape {x.shape}, y shape {y.shape}\")\n",
    "\n",
    "print(f\"Training pairs: {len(train_dataset)}\")\n",
    "print(f\"Validation pairs: {len(val_dataset)}\")\n",
    "\n",
    "model = UNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # load model onto device\n",
    "\n",
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=epochs,\n",
    "    checkpoint_path='vocal_isolator.pth'\n",
    ")"
   ],
   "id": "5ee739d80a880555",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset loading...\n",
      "Sample 0 : x shape torch.Size([2, 528, 864]), y shape torch.Size([2, 528, 864])\n",
      "Sample 1 : x shape torch.Size([2, 528, 864]), y shape torch.Size([2, 528, 864])\n",
      "Sample 2 : x shape torch.Size([2, 528, 864]), y shape torch.Size([2, 528, 864])\n",
      "Training pairs: 80\n",
      "Validation pairs: 20\n",
      "Starting training loop...\n",
      "Epoch 1/20 starting...\n",
      "setting to training mode\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.060000000000000005\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.09999999999999999\n",
      "0.10999999999999999\n",
      "0.11999999999999998\n",
      "0.12999999999999998\n",
      "0.13999999999999999\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18000000000000002\n",
      "0.19000000000000003\n",
      "0.20000000000000004\n",
      "0.21000000000000005\n",
      "0.22000000000000006\n",
      "0.23000000000000007\n",
      "0.24000000000000007\n",
      "0.25000000000000006\n",
      "0.26000000000000006\n",
      "0.2700000000000001\n",
      "0.2800000000000001\n",
      "0.2900000000000001\n",
      "0.3000000000000001\n",
      "0.3100000000000001\n",
      "0.3200000000000001\n",
      "0.3300000000000001\n",
      "0.34000000000000014\n",
      "0.35000000000000014\n",
      "0.36000000000000015\n",
      "0.37000000000000016\n",
      "0.38000000000000017\n",
      "0.3900000000000002\n",
      "0.4000000000000002\n",
      "0.4100000000000002\n",
      "0.4200000000000002\n",
      "0.4300000000000002\n",
      "0.4400000000000002\n",
      "0.45000000000000023\n",
      "0.46000000000000024\n",
      "0.47000000000000025\n",
      "0.48000000000000026\n",
      "0.49000000000000027\n",
      "0.5000000000000002\n",
      "0.5100000000000002\n",
      "0.5200000000000002\n",
      "0.5300000000000002\n",
      "0.5400000000000003\n",
      "0.5500000000000003\n",
      "0.5600000000000003\n",
      "0.5700000000000003\n",
      "0.5800000000000003\n",
      "0.5900000000000003\n",
      "0.6000000000000003\n",
      "0.6100000000000003\n",
      "0.6200000000000003\n",
      "0.6300000000000003\n",
      "0.6400000000000003\n",
      "0.6500000000000004\n",
      "0.6600000000000004\n",
      "0.6700000000000004\n",
      "0.6800000000000004\n",
      "0.6900000000000004\n",
      "0.7000000000000004\n",
      "0.7100000000000004\n",
      "0.7200000000000004\n",
      "0.7300000000000004\n",
      "0.7400000000000004\n",
      "0.7500000000000004\n",
      "0.7600000000000005\n",
      "0.7700000000000005\n",
      "0.7800000000000005\n",
      "0.7900000000000005\n",
      "0.8000000000000005\n",
      "Epoch 0/20 — Train Loss: 0.2028\n",
      "validation beginning\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "getting output from mixed for validation\n",
      "calculating loss\n",
      "calculating MAE\n",
      "calculating SDR\n",
      "Validation: Loss = 0.0860, MAE = 0.0860, SDR = 0.1506 dB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(2, 528, 864)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     20\u001B[39m criterion = torch.nn.L1Loss()\n\u001B[32m     21\u001B[39m optimizer = optim.Adam(model.parameters(), lr=\u001B[32m1e-3\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mvocal_isolator.pth\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     32\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, checkpoint_path)\u001B[39m\n\u001B[32m     67\u001B[39m     mixed_sample = mixed_sample.to(device)\n\u001B[32m     68\u001B[39m     predicted = model(mixed_sample[:\u001B[32m1\u001B[39m])\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     \u001B[43msave_prediction_as_xlsx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredicted\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpredicted_epoch_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.xlsx\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSaved sample prediction: predicted_epoch_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.xlsx\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     72\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining complete, model saved at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcheckpoint_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36msave_prediction_as_xlsx\u001B[39m\u001B[34m(prediction, filename)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msave_prediction_as_xlsx\u001B[39m(prediction, filename):\n\u001B[32m      7\u001B[39m     prediction_np = prediction.squeeze().cpu().detach().numpy()\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprediction_np\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     df.to_excel(filename, index=\u001B[38;5;28;01mFalse\u001B[39;00m, header=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Projects\\ENSC429_Group3\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001B[39m, in \u001B[36mDataFrame.__init__\u001B[39m\u001B[34m(self, data, index, columns, dtype, copy)\u001B[39m\n\u001B[32m    816\u001B[39m         mgr = dict_to_mgr(\n\u001B[32m    817\u001B[39m             \u001B[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001B[39;00m\n\u001B[32m    818\u001B[39m             \u001B[38;5;66;03m# attribute \"name\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    824\u001B[39m             copy=_copy,\n\u001B[32m    825\u001B[39m         )\n\u001B[32m    826\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m827\u001B[39m         mgr = \u001B[43mndarray_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    828\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    829\u001B[39m \u001B[43m            \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    830\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    831\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    832\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    833\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    834\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    836\u001B[39m \u001B[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001B[39;00m\n\u001B[32m    837\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(data):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Projects\\ENSC429_Group3\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:314\u001B[39m, in \u001B[36mndarray_to_mgr\u001B[39m\u001B[34m(values, index, columns, dtype, copy, typ)\u001B[39m\n\u001B[32m    312\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    313\u001B[39m         values = np.asarray(values)\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m     values = \u001B[43m_ensure_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    317\u001B[39m     \u001B[38;5;66;03m# by definition an array here\u001B[39;00m\n\u001B[32m    318\u001B[39m     \u001B[38;5;66;03m# the dtypes will be coerced to a single dtype\u001B[39;00m\n\u001B[32m    319\u001B[39m     values = _prep_ndarraylike(values, copy=copy_on_sanitize)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Projects\\ENSC429_Group3\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:592\u001B[39m, in \u001B[36m_ensure_2d\u001B[39m\u001B[34m(values)\u001B[39m\n\u001B[32m    590\u001B[39m     values = values.reshape((values.shape[\u001B[32m0\u001B[39m], \u001B[32m1\u001B[39m))\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m values.ndim != \u001B[32m2\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m592\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMust pass 2-d input. shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalues.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    593\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m values\n",
      "\u001B[31mValueError\u001B[39m: Must pass 2-d input. shape=(2, 528, 864)"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
